% ============================================================================
% SECCIÓN: DEPLOYMENT Y PRODUCCIÓN
%============================================================================

\section{Deployment y Producción}

\subsection{El Ecosistema de Inference Serving}

\begin{frame}{El Problema del Deployment}

    Poner un LLM en producción no es simplemente cargar el modelo y llamar \texttt{model.generate()}.

    \vspace{0.5cm}

    \begin{block}{Desafíos en producción}
        \begin{itemize}
            \item Manejar múltiples usuarios concurrentes
            \item Gestión eficiente de memoria (especialmente KV cache)
            \item Batching dinámico de requests
            \item Monitoreo y observabilidad
            \item Escalabilidad horizontal y vertical
        \end{itemize}
    \end{block}

    \vspace{0.3cm}

    Se necesitan sistemas especializados que optimicen estos aspectos.
\end{frame}

\begin{frame}{Métricas Críticas en Deployment}

    \textbf{Métricas clave para evaluar un sistema de serving:}

    \begin{itemize}
        \item \textbf{TTFT (Time To First Token):}
        \begin{itemize}
            \item Latencia percibida por el usuario
            \item Cuánto tarda en empezar a generar
            \item Crítico para experiencia de usuario
        \end{itemize}

        \vspace{0.2cm}

        \item \textbf{Throughput (tokens/segundo):}
        \begin{itemize}
            \item Capacidad total del sistema
            \item Cuántos tokens puede generar por segundo
            \item Crítico para escalar a múltiples usuarios
        \end{itemize}

        \vspace{0.2cm}

        \item \textbf{Requests concurrentes soportados:}
        \begin{itemize}
            \item Escalabilidad del sistema
        \end{itemize}

        \vspace{0.2cm}

        \item \textbf{Costo por token:}
        \begin{itemize}
            \item Eficiencia económica
        \end{itemize}
    \end{itemize}
\end{frame}

\subsection{Bibliotecas de Serving}

\begin{frame}{vLLM: Líder en Throughput}
    \textbf{vLLM:} El líder actual en throughput y eficiencia de memoria

    \vspace{0.3cm}

    \begin{itemize}
        \item \textbf{Innovación clave: PagedAttention}
        \begin{itemize}
            \item Maneja KV cache como memoria virtual del sistema operativo
            \item Reduce fragmentación dramáticamente
            \item Permite servir más requests concurrentes
        \end{itemize}

        \vspace{0.2cm}

        \item \textbf{Ventajas:}
        \begin{itemize}
            \item Hasta 24× mejor throughput que HuggingFace Transformers
            \item Continuous batching inteligente
            \item Soporte para 100+ arquitecturas
        \end{itemize}

        \vspace{0.2cm}

        \item \textbf{Trade-off:} TTFT ligeramente mayor

        \vspace{0.2cm}

        \item \textbf{Casos de uso:} APIs de producción con alto tráfico concurrente
    \end{itemize}

    \vspace{0.3cm}

    \begin{block}{Recomendación}
        \alert{vLLM es el estándar de facto} para serving LLMs en producción cuando el throughput es crítico.
    \end{block}
\end{frame}

\begin{frame}{Text Generation Inference (TGI)}
    \textbf{TGI de Hugging Face:}

    \vspace{0.3cm}

    \begin{itemize}
        \item \textbf{Ventajas:}
        \begin{itemize}
            \item Integración perfecta con ecosistema HuggingFace
            \item Telemetría robusta (OpenTelemetry, Prometheus)
            \item Docker images pre-built
            \item Muy fácil de deployar
        \end{itemize}

        \vspace{0.3cm}

        \item \textbf{Trade-off:} Throughput ligeramente menor que vLLM

        \vspace{0.3cm}

        \item \textbf{Casos de uso:}
        \begin{itemize}
            \item Equipos usando Hugging Face
            \item Necesidad de observabilidad completa
            \item Deployment en Kubernetes
        \end{itemize}
    \end{itemize}

    \vspace{0.3cm}

    \begin{block}{Cuándo elegir TGI}
        Si ya usas Hugging Face y valoras la facilidad de integración sobre el máximo throughput.
    \end{block}
\end{frame}

\begin{frame}{TensorRT-LLM y Ollama}

    \begin{columns}[T]
        \begin{column}{.48\textwidth}
            \textbf{TensorRT-LLM (NVIDIA)}

            \vspace{0.2cm}

            \begin{itemize}
                \item \textbf{Ventajas:}
                \begin{itemize}
                    \item Máximo rendimiento en hardware NVIDIA
                    \item Optimizaciones a nivel CUDA
                    \item Ultra-baja latencia
                \end{itemize}

                \vspace{0.2cm}

                \item \textbf{Trade-offs:}
                \begin{itemize}
                    \item Requiere compilación del modelo
                    \item Solo NVIDIA GPUs
                    \item Curva de aprendizaje más alta
                \end{itemize}

                \vspace{0.2cm}

                \item \textbf{Caso de uso:} Cuando ultra-baja latencia es requerida
            \end{itemize}
        \end{column}

        \begin{column}{.48\textwidth}
            \textbf{Ollama}

            \vspace{0.2cm}

            \begin{itemize}
                \item \textbf{Ventajas:}
                \begin{itemize}
                    \item Setup en una línea
                    \item Ideal para desarrollo local
                    \item Muy simple de usar
                \end{itemize}

                \vspace{0.2cm}

                \item \textbf{Trade-offs:}
                \begin{itemize}
                    \item No optimizado para producción multi-usuario
                    \item Throughput limitado
                \end{itemize}

                \vspace{0.2cm}

                \item \textbf{Casos de uso:}
                \begin{itemize}
                    \item Desarrollo local
                    \item Prototipos
                    \item Demos
                \end{itemize}
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\subsection{Conceptos Clave de Deployment}

\begin{frame}{Continuous Batching}

    \textbf{El problema del batching tradicional:}
    \begin{itemize}
        \item Esperar a llenar un batch completo antes de procesar
        \item Agrega latencia innecesaria
        \item Desperdicia recursos cuando el tráfico es irregular
    \end{itemize}

    \vspace{0.3cm}

    \textbf{Continuous Batching (usado por vLLM):}
    \begin{itemize}
        \item Procesa requests conforme llegan
        \item No espera a llenar un batch
        \item Agrega/remueve requests dinámicamente del batch
        \item Reduce latencia dramáticamente
    \end{itemize}

    \vspace{0.3cm}

    \begin{block}{Impacto}
        Continuous batching puede reducir latencia en \alert{5-10×} comparado con batching estático, especialmente con tráfico variable.
    \end{block}
\end{frame}

\begin{frame}{PagedAttention: El Secreto de vLLM}

    \textbf{El problema del KV Cache:}
    \begin{itemize}
        \item Crece con cada token generado
        \item Se fragmenta en memoria
        \item Desperdicia mucho espacio
        \item Limita cuántos requests puedes servir
    \end{itemize}

    \vspace{0.3cm}

    \textbf{PagedAttention (inspirado en memoria virtual de OS):}
    \begin{itemize}
        \item Divide KV cache en "páginas" pequeñas
        \item Almacena páginas no contiguamente
        \item Reduce fragmentación casi a cero
        \item Permite compartir páginas entre requests (prefix caching)
    \end{itemize}

    \vspace{0.3cm}

    \begin{alertblock}{Resultado}
        PagedAttention permite servir \alert{2-4× más requests concurrentes} con la misma memoria.
    \end{alertblock}
\end{frame}

\begin{frame}{Tensor Parallelism para Modelos Grandes}

    \textbf{El problema:}
    \begin{itemize}
        \item Modelos grandes no caben en una sola GPU
        \item Ejemplo: Llama 3.1 70B en FP16 requiere ~168 GB
        \item Una A100 solo tiene 80 GB
    \end{itemize}

    \vspace{0.3cm}

    \textbf{Solución: Tensor Parallelism}
    \begin{itemize}
        \item Divide el modelo entre múltiples GPUs
        \item Cada GPU procesa una parte de cada operación
        \item Sincronización constante entre GPUs
    \end{itemize}

    \vspace{0.3cm}

    \textbf{Trade-off:}
    \begin{itemize}
        \item [\alert{$+$}] Permite servir modelos que no cabrían en una GPU
        \item [\alert{$-$}] Overhead de comunicación entre GPUs
        \item [\alert{$-$}] Requiere interconexión rápida (NVLink, InfiniBand)
    \end{itemize}
\end{frame}

\begin{frame}{Comparación de Bibliotecas de Serving}

    \begin{table}[h!]
    \centering
    \tiny
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Biblioteca} & \textbf{Throughput} & \textbf{TTFT} & \textbf{Facilidad} & \textbf{Caso de uso} \\ \hline
    \textbf{vLLM} & Excelente & Bueno & Media & Producción alto tráfico \\ \hline
    \textbf{TGI} & Muy bueno & Muy bueno & Alta & Ecosistema HuggingFace \\ \hline
    \textbf{TensorRT-LLM} & Excelente & Excelente & Baja & Ultra-baja latencia \\ \hline
    \textbf{Ollama} & Básico & Bueno & Muy alta & Desarrollo local \\ \hline
    \textbf{HF Transformers} & Básico & Bueno & Muy alta & Prototipos, research \\ \hline
    \end{tabular}
    \end{table}

    \vspace{0.3cm}

    \begin{block}{Recomendación general}
        \begin{itemize}
            \item \textbf{Producción:} vLLM o TGI
            \item \textbf{Desarrollo local:} Ollama
            \item \textbf{Research/Prototipos:} HuggingFace Transformers
            \item \textbf{Ultra-optimización:} TensorRT-LLM
        \end{itemize}
    \end{block}
\end{frame}
