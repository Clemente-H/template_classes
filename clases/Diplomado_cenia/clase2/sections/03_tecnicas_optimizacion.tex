% ============================================================================
% SECCIÓN: TÉCNICAS DE OPTIMIZACIÓN Y EFICIENCIA
%============================================================================

\section{Técnicas de Optimización y Eficiencia}

\subsection{El Problema de la Accesibilidad}

\begin{frame}{¿Qué Hacer Cuando No Tenemos Todos Estos Recursos?}

    \begin{block}{El problema}
    Modelos grandes requieren recursos que no siempre tenemos disponibles. Necesitamos técnicas que reduzcan el costo computacional sin sacrificar demasiado la calidad.
    \end{block}

    \textbf{Principales técnicas de optimización}:
    \begin{enumerate}
        \item \textbf{Quantization}: Reduce precisión de pesos (FP16 $\rightarrow$ INT8 $\rightarrow$ INT4)
        \item \textbf{LoRA (Low-Rank Adaptation)}: Reduce parámetros entrenables 10-100$\times$
        \item \textbf{QLoRA}: Combina quantization + LoRA para máxima eficiencia
        \item \textbf{Pruning}: Elimina pesos/neuronas menos importantes
    \end{enumerate}

    \vspace*{.2cm}
    \begin{alertblock}{Trade-off fundamental}
    Todas estas técnicas intercambian \textbf{eficiencia} por (potencialmente) menor \textbf{calidad}. El arte está en minimizar la pérdida de calidad.
    \end{alertblock}

\end{frame}

\subsection{Quantization}

\begin{frame}[fragile]{Quantization: Reducir Precisión Inteligentemente}

    \begin{block}{Concepto}
    Quantization es el proceso de representar los pesos del modelo con menos bits, reduciendo drásticamente el uso de memoria y acelerando las operaciones.
    \end{block}

    \textbf{Niveles de precisión}:
    \begin{table}[h!]
    \centering
    \small
    \begin{tabular}{|l|c|c|c|}
    \hline
    \textbf{Formato} & \textbf{Bits} & \textbf{Reducción} & \textbf{Pérdida calidad} \\ \hline
    FP32 & 32 & 1$\times$ & Baseline \\ \hline
    FP16 & 16 & 2$\times$ & $<$1\% \\ \hline
    INT8 & 8 & 4$\times$ & 1-3\% \\ \hline
    INT4 & 4 & 8$\times$ & 3-8\% \\ \hline
    \end{tabular}
    \end{table}

    \begin{figure}
        \centering
        \includegraphics[width=0.6\linewidth]{GenIA/quantization.png}
        \caption{Representación visual de diferentes niveles de quantization}
    \end{figure}

\end{frame}

\begin{frame}[fragile]{Quantization: Tipos de Técnicas}

    \textbf{1. Post-Training Quantization (PTQ)}:
    \begin{itemize}
        \item Cuantiza un modelo ya entrenado \textbf{sin reentrenamiento}
        \item Rápido y simple, pero puede degradar performance
        \item \textbf{GPTQ}: Optimal Brain Quantization para LLMs, minimiza error layer-by-layer
        \item \textbf{AWQ} (Activation-aware Weight Quantization): Protege pesos importantes
        \item \textbf{SmoothQuant}: Suaviza outliers en activaciones para INT8 más estable
    \end{itemize}

    \vspace*{.2cm}
    \textbf{2. Quantization-Aware Training (QAT)}:
    \begin{itemize}
        \item Incorpora quantization durante el entrenamiento
        \item Modelo aprende representaciones robustas a cuantización
        \item Mejor calidad final pero más costoso (requiere acceso al training)
        \item Usado en modelos de Google (Gemma, PaLM)
    \end{itemize}

\end{frame}

\begin{frame}[fragile]{Quantization: Ejemplo Práctico con Llama 3.1 8B}

    \begin{columns}
    \column{0.48\textwidth}
    \textbf{Sin Quantization (FP16)}:
    \begin{itemize}
        \item Memoria: 19.2 GB
        \item Hardware: RTX 4090
        \item Velocidad: baseline
        \item Calidad: 100\%
    \end{itemize}

    \vspace*{.3cm}
    \textbf{INT8 Quantization}:
    \begin{itemize}
        \item Memoria: \alert{9.6 GB} (2$\times$ reducción)
        \item Hardware: RTX 3080
        \item Velocidad: 1.5-2$\times$ más rápido
        \item Calidad: $\sim$98-99\%
    \end{itemize}

    \column{0.48\textwidth}
    \textbf{INT4 Quantization}:
    \begin{itemize}
        \item Memoria: \alert{4.8 GB} (4$\times$ reducción)
        \item Hardware: RTX 3060
        \item Velocidad: 2-3$\times$ más rápido
        \item Calidad: $\sim$92-95\%
    \end{itemize}

    \vspace*{.3cm}
    \begin{alertblock}{Trade-off}
    Mayor reducción = mayor speedup pero potencial pérdida de calidad. \alert{INT8 es el "sweet spot"} para la mayoría de casos.
    \end{alertblock}

    \end{columns}

\end{frame}

\subsection{LoRA: Low-Rank Adaptation}

\begin{frame}[fragile]{LoRA: Low-Rank Adaptation}

    \begin{block}{La idea central}
    No necesitamos actualizar TODOS los parámetros durante fine-tuning. Las matrices de pesos tienen estructura redundante que podemos aproximar con matrices de rango bajo.
    \end{block}

    \textbf{Descomposición de rango bajo}:
    \[
    W = W_{\text{original}} + \Delta W \quad \text{donde} \quad \Delta W = B \times A
    \]

    Donde:
    \begin{itemize}
        \item $W$: matriz original de dimensión $d \times k$
        \item $B$: matriz de rango bajo $r$, dimensión $d \times r$
        \item $A$: matriz de rango bajo $r$, dimensión $r \times k$
        \item $r \ll \min(d,k)$ (típicamente $r=8, 16, 32$)
    \end{itemize}

\end{frame}

\begin{frame}[fragile]{LoRA: Visualización de la Arquitectura}

    \begin{figure}
        \centering
        \includegraphics[width=0.85\linewidth]{GenIA/lora.png}
        \caption{Arquitectura LoRA: matriz grande $W$ vs matrices pequeñas $B$ y $A$. Solo entrenamos $B$ y $A$, manteniendo $W$ frozen.}
    \end{figure}

\end{frame}

\begin{frame}[fragile]{LoRA: Matemática de la Eficiencia}

    \textbf{Reducción de parámetros}:
    \begin{itemize}
        \item Parámetros originales: $d \times k$
        \item Parámetros LoRA: $d \times r + r \times k = r(d+k)$
    \end{itemize}

    \textbf{Ejemplo concreto} para $d=k=4096$, $r=16$:
    \begin{itemize}
        \item Original: $4096 \times 4096 = 16{,}777{,}216$ parámetros
        \item LoRA: $16 \times (4096+4096) = 131{,}072$ parámetros
        \item \textbf{Reducción}: \alert{128$\times$} menos parámetros
    \end{itemize}

    \vspace*{.2cm}
    \begin{block}{Beneficios prácticos}
    \begin{itemize}
        \item Solo entrenas $\sim$0.1-1\% de los parámetros
        \item Memoria de entrenamiento: reducción de 10-100$\times$
        \item Velocidad de entrenamiento: 2-3$\times$ más rápido
        \item Múltiples adaptadores: puedes tener varios LoRA para diferentes tareas
    \end{itemize}
    \end{block}

\end{frame}

\begin{frame}[fragile]{LoRA: Limitaciones y Cuándo Usar}

    \textbf{Limitaciones}:
    \begin{itemize}
        \item Menor expresividad que full fine-tuning
        \item Mejor para \textit{adaptation} que para aprender conocimiento completamente nuevo
        \item Requiere tuning del hiperparámetro $r$ (rank)
        \item No todos los layers se benefician igual de LoRA
    \end{itemize}

    \vspace*{.2cm}
    \textbf{Cuándo usar LoRA}:
    \begin{itemize}
        \item Fine-tuning con recursos limitados (1 GPU en lugar de cluster)
        \item Necesitas múltiples adaptadores para diferentes tareas
        \item El modelo base ya es bueno, solo necesitas ajustes
        \item Quieres iterar rápidamente con diferentes datasets
    \end{itemize}

\end{frame}

\subsection{QLoRA: La Combinación Perfecta}

\begin{frame}[fragile]{QLoRA: Lo Mejor de Dos Mundos}

    \begin{block}{La combinación perfecta}
    \begin{itemize}
        \item \textbf{Quantization} (INT4) para base model $\rightarrow$ reduce memoria 4$\times$
        \item \textbf{LoRA} para fine-tuning $\rightarrow$ solo entrenas $\sim$0.1\% params
        \item \textbf{Resultado}: Fine-tune 70B en una GPU de 48GB
    \end{itemize}
    \end{block}

    \begin{figure}
        \centering
        \includegraphics[width=0.75\linewidth]{GenIA/qlora.jpg}
        \caption{Arquitectura QLoRA: modelo base cuantizado (frozen) + adaptadores LoRA entrenables}
    \end{figure}

\end{frame}

\begin{frame}[fragile]{QLoRA: Innovaciones Clave}

    \textbf{1. 4-bit NormalFloat (NF4)}:
    \begin{itemize}
        \item Distribución especial de quantization bins
        \item Optimal para pesos de redes neuronales (distribución normal)
        \item Mejor que INT4 uniforme para model weights
    \end{itemize}

    \vspace*{.2cm}
    \textbf{2. Double Quantization}:
    \begin{itemize}
        \item Quantiza los \textit{quantization constants} también
        \item Ahorra $\sim$0.4 bits por parámetro adicional
        \item Reduce overhead de metadata
    \end{itemize}

    \vspace*{.2cm}
    \textbf{3. Paged Optimizers}:
    \begin{itemize}
        \item Usa CPU RAM como backup para optimizer states
        \item Evita OOM (Out-Of-Memory) errors en secuencias largas
        \item Transparente para el usuario
    \end{itemize}

\end{frame}

\begin{frame}[fragile]{QLoRA: Resultados Empíricos}

    \textbf{Resultados del paper original:}

    \begin{itemize}
        \item QLoRA 65B $\approx$ Full fine-tuning 65B en benchmarks
        \item Llama 65B fine-tuned en $\sim$48GB VRAM (vs $\sim$800GB para full FT)
        \item Guanaco (Llama+QLoRA) competitive con ChatGPT 3.5
        \item Minimal quality loss: $<$2\% en MMLU, HumanEval
    \end{itemize}

    \vspace*{.3cm}
    \textbf{Trade-offs de QLoRA}:
    \begin{itemize}
        \item [\alert{$+$}] Calidad comparable a full fine-tuning
        \item [\alert{$+$}] Democratiza el fine-tuning de modelos grandes
        \item [\alert{$+$}] Permite experimentación rápida sin cluster de GPUs
        \item [\alert{$-$}] Entrenamiento ligeramente más lento que full fine-tuning
    \end{itemize}

\end{frame}

\subsection{Otras Técnicas}

\begin{frame}{Pruning: Eliminando lo Innecesario}
    \begin{block}{Concepto Básico}
        El \textit{pruning} (o poda) consiste en \textbf{identificar y eliminar los pesos o neuronas menos importantes} de un modelo ya entrenado.
    \end{block}

    \vspace{0.3cm}

    \begin{columns}[T]
        \begin{column}{.48\textwidth}
            \textbf{Pruning No Estructurado}
            \begin{itemize}
                \item Elimina pesos individuales
                \item Genera matrices "escasas" (sparse)
                \item Problema: GPUs no son eficientes con matrices sparse
                \item No se consigue aceleración real sin hardware especializado
            \end{itemize}
        \end{column}

        \begin{column}{.48\textwidth}
            \textbf{Pruning Estructurado}
            \begin{itemize}
                \item Elimina grupos enteros: neuronas, canales, heads
                \item Matrices densas pero más pequeñas
                \item \textbf{Ventaja}: aceleración real en hardware estándar
            \end{itemize}
        \end{column}
    \end{columns}

    \vspace{0.3cm}

    \begin{alertblock}{Cuándo usar Pruning}
        Cuando tienes acceso al pipeline de entrenamiento y presupuesto para re-entrenar. Requiere más trabajo que quantization, pero puede dar resultados superiores.
    \end{alertblock}
\end{frame}

\begin{frame}[fragile]{Trade-offs: Tabla Comparativa de Técnicas}

    \begin{table}[h!]
    \centering
    \tiny
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Técnica} & \textbf{Ahorro Memoria} & \textbf{Speedup} & \textbf{Pérdida Calidad} & \textbf{Dificultad} \\ \hline
    \textbf{LoRA} & 10-100$\times$ (training) & $\sim$1$\times$ & Mínima & Media \\ \hline
    \textbf{INT8 Quant} & 2$\times$ & 1.5-2$\times$ & 1-3\% & Baja \\ \hline
    \textbf{INT4 Quant} & 4$\times$ & 2-3$\times$ & 3-8\% & Media \\ \hline
    \textbf{QLoRA} & 4$\times$ (training) & 0.5$\times$ (lento) & $\sim$2\% & Media \\ \hline
    \textbf{Pruning} & 2-4$\times$ & 1.5-3$\times$ & Variable & Alta \\ \hline
    \end{tabular}
    \end{table}

    \vspace*{.2cm}
    \textbf{Combinaciones efectivas}:
    \begin{itemize}
        \item INT8 + LoRA: fine-tuning eficiente de modelos grandes
        \item INT4 + LoRA (QLoRA): máxima eficiencia en recursos limitados
        \item INT8 para inferencia: mejor balance calidad/velocidad
    \end{itemize}

    \vspace*{.2cm}
    \begin{block}{Decision framework}
    \begin{enumerate}
        \item ¿Solo inferencia? $\rightarrow$ Quantization (INT8 o INT4)
        \item ¿Necesitas fine-tune? $\rightarrow$ LoRA o QLoRA
        \item ¿Hardware ultra limitado? $\rightarrow$ QLoRA con INT4
    \end{enumerate}
    \end{block}

\end{frame}
