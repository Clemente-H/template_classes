% ============================================================================
% SECCIÓN: RECAPITULACIÓN Y CIERRE
%============================================================================

\section{Recapitulación}

\begin{frame}{Resumen: El Flujo Completo}

    \begin{enumerate}
        \item \textbf{Hardware: Entender recursos necesarios}
        \begin{itemize}
            \item VRAM es el cuello de botella
            \item Usar calculadora: \url{https://vram.asmirnov.xyz}
            \item Training requiere 3-4× más memoria que inferencia
        \end{itemize}

        \vspace{0.2cm}

        \item \textbf{Optimización: Reducir requerimientos}
        \begin{itemize}
            \item Quantization (INT8 = sweet spot)
            \item LoRA (reduce parámetros entrenables 100×)
            \item QLoRA (democratiza fine-tuning de modelos grandes)
        \end{itemize}

        \vspace{0.2cm}

        \item \textbf{Deployment: Servir eficientemente}
        \begin{itemize}
            \item vLLM para producción con alto throughput
            \item TGI para ecosistema HuggingFace
            \item Ollama para desarrollo local
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{Mensajes Clave para Llevar}

    \begin{alertblock}{1. Pre-training vs Fine-tuning}
        \begin{itemize}
            \item Pre-training desde cero = inaccesible para la mayoría
            \item Fine-tuning con técnicas modernas = \alert{muy accesible}
        \end{itemize}
    \end{alertblock}

    \vspace{0.2cm}

    \begin{alertblock}{2. El "sweet spot" de optimización}
        \begin{itemize}
            \item \textbf{Inferencia:} INT8 quantization (balance perfecto)
            \item \textbf{Fine-tuning:} QLoRA (democratiza modelos grandes)
        \end{itemize}
    \end{alertblock}

    \vspace{0.2cm}

    \begin{alertblock}{3. Serving en producción}
        \begin{itemize}
            \item vLLM = estándar de facto para producción
            \item PagedAttention y Continuous Batching son game changers
        \end{itemize}
    \end{alertblock}
\end{frame}

\begin{frame}{Lo que Hicimos Posible}

    \textbf{Hace 2-3 años:}
    \begin{itemize}
        \item Fine-tune Llama 70B: Requería cluster de 10+ GPUs de datacenter
        \item Costo: \$10,000+ en cloud
        \item Solo accesible para grandes empresas
    \end{itemize}

    \vspace{0.5cm}

    \textbf{Hoy, con QLoRA:}
    \begin{itemize}
        \item Fine-tune Llama 70B: 1× GPU de 48GB
        \item Costo: \$100-200 en cloud (o tu RTX 4090 local)
        \item \alert{Accesible para estudiantes, startups, investigadores}
    \end{itemize}

    \vspace{0.5cm}

    \begin{block}{El poder de la optimización}
        Las técnicas de eficiencia no solo ahorran dinero, \alert{democratizan el acceso a la tecnología}.
    \end{block}
\end{frame}

\begin{frame}{Recursos Útiles}

    \textbf{Herramientas:}
    \begin{itemize}
        \item VRAM Calculator: \url{https://vram.asmirnov.xyz}
        \item vLLM: \url{https://github.com/vllm-project/vllm}
        \item Hugging Face TGI: \url{https://github.com/huggingface/text-generation-inference}
        \item Ollama: \url{https://ollama.ai}
    \end{itemize}

    \vspace{0.3cm}

    \textbf{Papers clave:}
    \begin{itemize}
        \item LoRA: Low-Rank Adaptation of Large Language Models (Hu et al., 2021)
        \item QLoRA: Efficient Finetuning of Quantized LLMs (Dettmers et al., 2023)
        \item Efficient Memory Management for LLM Serving with PagedAttention (Kwon et al., 2023)
    \end{itemize}

    \vspace{0.3cm}

    \textbf{Comunidad:}
    \begin{itemize}
        \item Hugging Face Hub: modelos pre-quantized, adaptadores LoRA
        \item r/LocalLLaMA (Reddit): comunidad de self-hosting LLMs
    \end{itemize}
\end{frame}

\begin{frame}
    \begin{center}
        \Huge ¡Gracias!
        \vspace{1cm}

        \Large ¿Preguntas?

        \vspace{1cm}

        \normalsize
        Próxima sesión: Práctica de fine-tuning con QLoRA
    \end{center}
\end{frame}
