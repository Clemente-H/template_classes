\section{Recursos y Referencias}

\begin{frame}[fragile]{Papers Fundamentales}
\begin{small}
  \textbf{Modelos open-source:}
  \begin{itemize}
    \item \textbf{Open-Sora}: \url{https://arxiv.org/pdf/2412.20404}
    \item HuggingFace: \url{https://huggingface.co/hpcai-tech/Open-Sora}
  \end{itemize}

  \vspace{0.3cm}

  \textbf{Sistemas propietarios (blogs/papers):}
  \begin{itemize}
    \item \textbf{V2A DeepMind}: \url{https://deepmind.google/blog/generating-audio-for-video/}
    \item \textbf{Sora (OpenAI)}: \url{https://openai.com/sora}
    \item \textbf{Veo (Google)}: \url{https://deepmind.google/technologies/veo/}
  \end{itemize}

  \vspace{0.3cm}

  \textbf{Fundamentos que ya vieron (recall):}
  \begin{itemize}
    \item \textbf{VALL-E}: \url{https://arxiv.org/abs/2301.02111}
    \item \textbf{Stable Video Diffusion}: \url{https://arxiv.org/abs/2311.15127}
    \item \textbf{VideoGPT}: \url{https://arxiv.org/abs/2104.10157}
  \end{itemize}
\end{small}
\end{frame}

\begin{frame}[fragile]{Recursos para Experimentar}
\begin{small}
  \textbf{Modelos que pueden probar:}
  \begin{itemize}
    \item \textbf{Open-Sora}: \url{https://github.com/hpcaitech/Open-Sora}
    \item \textbf{Stable Video Diffusion}: Disponible en Stability AI
    \item \textbf{MusicGen} (Meta): \url{https://musicgen.com/}
  \end{itemize}

  \vspace{0.3cm}

  \textbf{Demos interactivos:}
  \begin{itemize}
    \item Veo 3: \url{https://aistudio.google.com/models/veo-3}
    \item (Requieren acceso/waitlist)
  \end{itemize}

  \vspace{0.3cm}

  \textbf{Para profundizar:}
  \begin{itemize}
    \item \textbf{World models}: \url{https://www.worldlabs.ai/}
    \item \textbf{Fei-Fei Li on Spatial Intelligence}: \\
    \url{https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence}
  \end{itemize}
\end{small}
\end{frame}

\begin{frame}[fragile]{Cierre: Las Ideas Clave}
\begin{small}
  \textbf{Lo que aprendimos hoy:}

  \vspace{0.3cm}

  \begin{enumerate}
    \item \textbf{El problema fundamental}: Unir audio y video requiere más que buenos modelos individuales — requiere coherencia cross-modal

    \vspace{0.2cm}

    \item \textbf{Tres formas de condicionar}: V2A, A2V, T2AV — cada una con desafíos únicos

    \vspace{0.2cm}

    \item \textbf{Las soluciones modernas}: Diffusion Transformers, representaciones latentes, entrenamiento multi-modal

    \vspace{0.2cm}

    \item \textbf{Lo que falta}: Comprensión física, edición coherente, consistencia a largo plazo

    \vspace{0.2cm}

    \item \textbf{El futuro no es técnico}: Los desafíos son éticos, sociales, y de gobernanza
  \end{enumerate}

  \vspace{0.3cm}

  \begin{alertblock}{Reflexión final}
  Ya no preguntamos "¿se puede generar contenido audiovisual convincente?"

  La pregunta es: \textbf{¿Qué tipo de mundo estamos construyendo con estas herramientas?}
  \end{alertblock}
\end{small}
\end{frame}
